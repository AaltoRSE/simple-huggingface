{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd63e696-85de-4233-b5f8-2425c4037120",
   "metadata": {},
   "source": [
    "# Model inference with Hugging Face\n",
    "\n",
    "Nowadays it is very common to run inference with models released in the Hugging Face hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40062162-c155-4213-9334-388654b77648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Here environment variable WRKDIR points to a personal work directory\n",
    "os.environ[\"HF_HOME\"] = f\"{os.environ[\"WRKDIR\"]}/huggingface\"\n",
    "os.environ[\"HF_TOKEN_PATH\"] = \"~/.cache/huggingface/token\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c9e48-1e39-466c-b6fd-bbb6a08543bd",
   "metadata": {},
   "source": [
    "##  Hugging Face models\n",
    "\n",
    "Hugging Face models are [an abstraction](https://huggingface.co/docs/transformers/en/models) that describes a pretrained model that can be used for different use cases.\n",
    "\n",
    "Because different models have different structures they are all collected under [AutoModel](https://huggingface.co/docs/transformers/en/model_doc/auto) classes that abstract the model loading and usage.\n",
    "\n",
    "AutoModels are further extended by specialized classes that define how the model behaves with different tasks. Each model has supported tasks enabled by extending classes such as [AutoModelForCausalLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCausalLM), [AutoModelForQuestionAnswering](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForQuestionAnswering) or [AutoModelForImageClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForImageClassification).\n",
    "\n",
    "You can specify the model yourself by using [AutoModel.from_pretrained](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel.from_pretrained) or the function with the same name from the subclass e.g. [AutoModelForCausalLM.from_pretrained](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained), but using pipeline is usually easier.\n",
    "\n",
    "## Transformers pipeline\n",
    "\n",
    "Different tasks are enabled on different models. However, all AutoModels support Transformer's [pipeline API](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial) that makes launching models relatively simple. You just need to provide the pipeline with the task name and the model name and it will handle the rest.\n",
    "\n",
    "[Pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) itself has multiple subclasses like [TextGenerationPipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.TextGenerationPipeline) that define all steps needed to run the model from preprocessing to the result decoding.\n",
    "\n",
    "[Pipeline-class](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.Pipeline) takes huge number different options that can be used to modify the . When pipeline is created, many of these options are passed to various classes that pipeline creates.\n",
    "\n",
    "Returned Pipeline has all of the different things baked into one class:\n",
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "Pf[pipeline-function] --> P[Preprocessor chosen based on task]\n",
    "Pf --> T[Tokenizer chosen based on task]\n",
    "Pf --> A[AutoModel chosen based on task]\n",
    "Pf --> F[Framework chosen based on settings]\n",
    "P --> Pc[Pipeline-class]\n",
    "T --> Pc\n",
    "A --> Pc\n",
    "F --> Pc\n",
    "```\n",
    "\n",
    "When this class is called to do its task, it will run through the full pipeline:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "\n",
    "Pi[\"pipeline(data)\"] --> P[\"Preprocessor (e.g. AutoProcessor)\"]\n",
    "P --> Te[\"Tokenizer encoding (e.g. AutoTokenizer)\"]\n",
    "Te --> A[\"AutoModel (e.g. AutoModelForCausalLM)\"]\n",
    "A --> M[\"Model (e.g. mistralai/Mistral-7B-Instruct-v0.3)\"]\n",
    "M --> F[\"Framework (e.g. PyTorch)\"]\n",
    "F --> O[Model output]\n",
    "O --> Td[Tokenizer decoding]\n",
    "Td --> Pd[Pipeline output]\n",
    "```\n",
    "\n",
    "In practice all of this is just a couple lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d416eec8-85cf-4041-a0c4-3fb27868d494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7223c9a56a734195bdb8760ea0791b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882d5fa-f9ac-4f16-a20c-c38e44d855dd",
   "metadata": {},
   "source": [
    "Calling the pipeline is simple as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd48532f-7979-47f1-9074-b026db03e509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In 200 words or less, how do you make spaghetti bolognaise?\\n\\n1. Sauté onion, carrot, celery, and garlic in olive oil until soft.\\n2. Add ground meat, cook until browned.\\n3. Stir in tomato paste, canned tomatoes, beef broth, wine (optional), bay leaves, and thyme.\\n4. Simmer for 30 minutes, stirring occasionally.\\n5. Season with salt, pepper, and sugar to taste.\\n6. Cook spaghetti according to package instructions, drain.\\n7. Toss spaghetti with sauce, garnish with grated Parmesan cheese and fresh basil.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"In 200 words or less, how do you make spaghetti bolognaise?\", max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b400d-cb6b-4287-8fd5-c698f98c6320",
   "metadata": {},
   "source": [
    "We can silence the warning about the `pad_token_id` by using the same pad token for the model as we use for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541ce4b5-b7c1-47f4-8609-ca7f7622536d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In 200 words or less, how do you make spaghetti bolognaise?\\n\\nCook spaghetti according to package directions. For the bolognaise sauce, sauté onion, carrot, and celery in olive oil until tender. Add ground beef, cook until browned. Stir in tomato paste, crushed tomatoes, salt, pepper, and basil. Simmer for 20 minutes. Drain spaghetti and toss with sauce. Serve with grated Parmesan cheese. Enjoy!'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"In 200 words or less, how do you make spaghetti bolognaise?\", max_new_tokens=512, pad_token_id=pipe.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5da5c-2845-4834-88f7-f9cc91c21c0f",
   "metadata": {},
   "source": [
    "## Handling system prompts when using LLMs\n",
    "\n",
    "### Setting a system prompt\n",
    "\n",
    "LLMs that are fine-tuned for instruction answering use a system prompt that specifies how the model should behave. Typically system prompt contains ground truths and instructions that the LLM is expected to know and follow.\n",
    "\n",
    "If no system prompt is provided the model will use a default one (e.g. [this one for Llama based models](https://github.com/huggingface/transformers/blob/6b5bd117231f969713ed79fd4870903ab3c93edf/src/transformers/models/llama/tokenization_llama.py#L47)).\n",
    "\n",
    "However, in many cases overriding the system prompt is the best way of controlling the LLMs behaviour because the LLM has been trained to emphasize its contents.\n",
    "\n",
    "Changing the system prompt is done by specifying a message chain where the first message is a system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61652282-bc83-42a7-a60f-dcb29e726d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'system',\n",
       "    'content': \"You're a three start Michelin chef answering questions from the public. Answer in 200 words or less.\"},\n",
       "   {'role': 'user', 'content': 'How do you make spaghetti bolognaise?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': ' As a Michelin-starred chef, I aim to elevate classic dishes while maintaining their essence. My Spaghetti Bolognese recipe balances tradition with refinement.\\n\\nIngredients:\\n1. 500g high-quality ground beef\\n2. 1 large yellow onion, finely chopped\\n3. 2 carrots, finely chopped\\n4. 2 celery stalks, finely chopped\\n5. 4 cloves garlic, minced\\n6. 1 cup red wine (preferably a full-bodied Italian variety)\\n7. 1 can (400g) san Marzano tomatoes\\n8. 500g spaghetti\\n9. Extra-virgin olive oil\\n10. Salt and freshly ground black pepper, to taste\\n11. Parmesan cheese, grated (optional)\\n\\nInstructions:\\n1. Heat oil in a large pan over medium heat. Add onion, carrots, celery, and garlic, cooking until softened.\\n2. Add ground beef, season with salt and pepper, and cook until browned.\\n3. Pour in red wine, allowing it to reduce slightly.\\n4. Add tomatoes and their juice, simmering the sauce for at least 1 hour, stirring occasionally.\\n5. Cook spaghetti according to package instructions, then drain.\\n6. Toss cooked spaghetti in sauce, serving with a sprinkle of Parmesan cheese, if desired.\\n\\nEnjoy this authentic Italian dish with a glass of Barolo or Chianti for the perfect pairing. Buon appetito!'}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You're a three start Michelin chef answering questions from the public. Answer in 200 words or less.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do you make spaghetti bolognaise?\"},\n",
    "]\n",
    "\n",
    "pipe(messages, max_new_tokens=512, pad_token_id=pipe.tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcac97-f51c-4131-8271-a1731a30e389",
   "metadata": {},
   "source": [
    "### Reusing the system prompt\n",
    "\n",
    "Sometimes you'll want to reuse the same system prompt for multiple messages. An easy way of achieving this by creating a small helper function that injects the system prompt for every message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb1eb14-f89b-4d9f-a181-0b97f0ca6977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'role': 'system', 'content': \"You're a three start Michelin chef answering questions from the public. Answer in 200 words or less.\"}, {'role': 'user', 'content': 'How do you make spaghetti bolognaise?'}, {'role': 'assistant', 'content': \" Creating a delectable Spaghetti Bolognese requires a harmonious blend of quality ingredients and patience. Here's a simplified recipe:\\n\\n1. Gently sauté finely chopped onion, carrot, and celery in olive oil until they soften. Season with salt and pepper.\\n\\n2. Add ground beef, breaking it up with a wooden spoon. Cook until browned, then drain off any excess fat.\\n\\n3. Stir in a handful of tomato paste and cook for a minute or two. This will help develop a richer flavor.\\n\\n4. Pour in a can of whole peeled tomatoes, crushed by hand. Add a glass of red wine if desired, and let it simmer for about 30 minutes.\\n\\n5. Meanwhile, cook your spaghetti al dente according to package instructions.\\n\\n6. Stir in grated Parmesan cheese and fresh basil leaves. Taste and adjust seasoning if necessary.\\n\\n7. Serve your Spaghetti Bolognese hot, with a dusting of Parmesan on top if desired. Buon appetito!\"}]}]\n",
      "[{'generated_text': [{'role': 'system', 'content': \"You're a three start Michelin chef answering questions from the public. Answer in 200 words or less.\"}, {'role': 'user', 'content': 'How do you make a cake?'}, {'role': 'assistant', 'content': \" As a three-star Michelin chef, I pride myself on the precision and creativity in my culinary creations. Here's a simplified version of a classic sponge cake recipe, which you can adapt and elevate to your taste.\\n\\nIngredients:\\n1. 120g unsalted butter, softened\\n2. 120g caster sugar\\n3. 2 large eggs\\n4. 120g self-raising flour\\n5. 1 tsp baking powder\\n6. 2 tbsp milk\\n7. 1 tsp vanilla extract (optional)\\n\\nInstructions:\\n1. Preheat the oven to 180°C (160°C fan) / Gas Mark 4 / 350°F. Grease and line an 18cm square cake tin.\\n2. In a large mixing bowl, cream together the butter and sugar until light and fluffy.\\n3. Beat in the eggs, one at a time, followed by the vanilla extract.\\n4. Sift in the flour and baking powder, then fold them gently into the mixture.\\n5. Gradually add the milk while continuing to fold, ensuring the mixture remains smooth.\\n6. Pour the batter into the prepared cake tin and spread it evenly.\\n7. Bake for 20-25 minutes, or until a toothpick inserted into the center of the cake comes out clean.\\n8. Allow the cake to cool in the tin for 10 minutes, then transfer it to a wire rack to cool completely.\\n\\nYou can decorate this cake with your favorite frosting, fruit, or chocolate ganache. Bon appétit!\"}]}]\n"
     ]
    }
   ],
   "source": [
    "def prompt_creator(system_prompt=\"\", messages=None):\n",
    "    if not messages:\n",
    "        messages = []\n",
    "    for message in messages:\n",
    "        yield [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ]\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt_creator(\n",
    "        system_prompt=\"You're a three start Michelin chef answering questions from the public. Answer in 200 words or less.\",\n",
    "        messages=[\"How do you make spaghetti bolognaise?\", \"How do you make a cake?\"]\n",
    "    ), max_new_tokens=512, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8ca1c-19dc-4671-b142-a330aff58ca6",
   "metadata": {},
   "source": [
    "## Batch inference\n",
    "\n",
    "Pipeline also supports [batch inference](https://huggingface.co/docs/transformers/main/en/pipeline_tutorial#large-datasets), but its performance depends heavily on the data ingestion.\n",
    "\n",
    "If the data is provided in an optimal form i.e. as datasets, then pipeline can automatically convert the data into batched tensors.\n",
    "\n",
    "This can be achieved by creating a dataset out of the data and doing the system prompt creation for the whole dataset. Pipeline will automatically convert the dataset into a efficient batches.\n",
    "\n",
    "Lets consider a [stanfordnlp/imdb](https://huggingface.co/datasets/stanfordnlp/imdb)-dataset that contains reviews of movies and the perceived sentiment of those reviews (positive or negative).\n",
    "\n",
    "Let's take a subset of reviews designed for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca307a8-5c53-4818-b633-a43bfe41b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a761d93-da4e-4c07-9628-e9d77e8ac97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")['train'].shuffle().take(n_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d2f10-c962-48d6-8bfa-6476b372779f",
   "metadata": {},
   "source": [
    "Each review contains a `text`-portion and a `label`-portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce243587-43c5-4746-8e19-8463cf059516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Skip McCoy is a three time loser pick pocket, unable to curb his instincts back on the street, he picks the purse of Candy on a subway train. What he doesn't realise is that Candy is carrying top secret microfilm, microfilm that is of high interest to many many organisations.<br /><br />Director Samuel Fuller has crafted an exceptional drama set amongst the seedy underworld of New York City. Communist spies and shady government operatives all blend together to make Pickup On South Street a riveting viewing from first minute to the last. Based around a Dwight Taylor story called Blaze Of Glory, Fuller enthused this adaptation with heavy set political agenda, something that many at the time felt was over done, but to only focus on its anti communist leanings is doing it a big disservice.<br /><br />Digging a little deeper and you find characters as intriguing as any that Fuller has directed, the main protagonist for one is the hero of the piece, a crook and a shallow human being, his heroics are not born out of love for his country, they are born out of his sheer stubborn streak. It's quite an achievement that Fuller has crafted one of the best anti heroes of the 50s, and i'm sure he was most grateful to the performance of Richard Widmark as McCoy, all grin and icy cold heart, his interplay with the wonderful Jean Peters as Candy is excellent, and is the films heart. However it is the Oscar nominated Thelma Ritter who takes the acting honours, her Moe is strong and as seedy as the surrounding characters, but there is a tired warmth to her that Ritter conveys majestically.<br /><br />It's a B movie in texture but an A film in execution, Pickup On South Street is a real classy and entertaining film that is the best of its most intriguing director. 9/10\",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773013f7-9c97-4136-b112-de82b1f244ff",
   "metadata": {},
   "source": [
    "For sentiment analysis, we need to switch out the system prompt and we can do it by creating a function that processes a review and returns corresponding messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7413d3b1-004d-459e-b1d4-9020216d5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(review):\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You're an accurate classification engine designed to determine if a movie review has a positive or negative sentiment.\n",
    "    \n",
    "    Reviews can be positive or negative. You're given a review and you will respond with 0 if the sentiment of the review is negative and 1 if the review is positive.\n",
    "    \n",
    "    Give no other output.\n",
    "    \"\"\"\n",
    "    \n",
    "    return {\n",
    "        'messages': [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": review['text']}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003ff1d-c931-494c-83ab-9a67c3407253",
   "metadata": {},
   "source": [
    "Now that we have a data preprocessing function, we can map it over the dataset to create messages we want to pass to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5afa33de-614a-49ed-a821-72820077fab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ad1f37a4bd4807bc1c60af45006a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(data_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d8a27f-2fbf-44c6-b378-32fb10bcb2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Skip McCoy is a three time loser pick pocket, unable to curb his instincts back on the street, he picks the purse of Candy on a subway train. What he doesn't realise is that Candy is carrying top secret microfilm, microfilm that is of high interest to many many organisations.<br /><br />Director Samuel Fuller has crafted an exceptional drama set amongst the seedy underworld of New York City. Communist spies and shady government operatives all blend together to make Pickup On South Street a riveting viewing from first minute to the last. Based around a Dwight Taylor story called Blaze Of Glory, Fuller enthused this adaptation with heavy set political agenda, something that many at the time felt was over done, but to only focus on its anti communist leanings is doing it a big disservice.<br /><br />Digging a little deeper and you find characters as intriguing as any that Fuller has directed, the main protagonist for one is the hero of the piece, a crook and a shallow human being, his heroics are not born out of love for his country, they are born out of his sheer stubborn streak. It's quite an achievement that Fuller has crafted one of the best anti heroes of the 50s, and i'm sure he was most grateful to the performance of Richard Widmark as McCoy, all grin and icy cold heart, his interplay with the wonderful Jean Peters as Candy is excellent, and is the films heart. However it is the Oscar nominated Thelma Ritter who takes the acting honours, her Moe is strong and as seedy as the surrounding characters, but there is a tired warmth to her that Ritter conveys majestically.<br /><br />It's a B movie in texture but an A film in execution, Pickup On South Street is a real classy and entertaining film that is the best of its most intriguing director. 9/10\",\n",
       " 'label': 1,\n",
       " 'messages': [{'content': \"\\n    You're an accurate classification engine designed to determine if a movie review has a positive or negative sentiment.\\n\\n    Reviews can be positive or negative. You're given a review and you will respond with 0 if the sentiment of the review is negative and 1 if the review is positive.\\n\\n    Give no other output.\\n    \",\n",
       "   'role': 'system'},\n",
       "  {'content': \"Skip McCoy is a three time loser pick pocket, unable to curb his instincts back on the street, he picks the purse of Candy on a subway train. What he doesn't realise is that Candy is carrying top secret microfilm, microfilm that is of high interest to many many organisations.<br /><br />Director Samuel Fuller has crafted an exceptional drama set amongst the seedy underworld of New York City. Communist spies and shady government operatives all blend together to make Pickup On South Street a riveting viewing from first minute to the last. Based around a Dwight Taylor story called Blaze Of Glory, Fuller enthused this adaptation with heavy set political agenda, something that many at the time felt was over done, but to only focus on its anti communist leanings is doing it a big disservice.<br /><br />Digging a little deeper and you find characters as intriguing as any that Fuller has directed, the main protagonist for one is the hero of the piece, a crook and a shallow human being, his heroics are not born out of love for his country, they are born out of his sheer stubborn streak. It's quite an achievement that Fuller has crafted one of the best anti heroes of the 50s, and i'm sure he was most grateful to the performance of Richard Widmark as McCoy, all grin and icy cold heart, his interplay with the wonderful Jean Peters as Candy is excellent, and is the films heart. However it is the Oscar nominated Thelma Ritter who takes the acting honours, her Moe is strong and as seedy as the surrounding characters, but there is a tired warmth to her that Ritter conveys majestically.<br /><br />It's a B movie in texture but an A film in execution, Pickup On South Street is a real classy and entertaining film that is the best of its most intriguing director. 9/10\",\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d89e6-9ae7-477a-8583-7b2a362ba942",
   "metadata": {},
   "source": [
    "We only want to pass the `messages`-values to the pipeline and we can use an utility class called `KeyDataset` to pick only values that are under the `messages`-key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e9b4b58-c7b0-4dea-ac28-ca765fd6a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0f3bc-ef7f-416c-867f-06869729fd1f",
   "metadata": {},
   "source": [
    "Now we can run and time the pipeline execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ceb976-7774-41ef-9882-8c3d9a4f063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.2 s, sys: 847 ms, total: 28 s\n",
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentiments = [\n",
    "    sentiment[0]['generated_text'][-1]['content']\n",
    "    for sentiment in pipe(\n",
    "            KeyDataset(ds, 'messages'),\n",
    "            pad_token_id=pipe.tokenizer.eos_token_id\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde8246-6722-409e-9e6c-1589754cedf9",
   "metadata": {},
   "source": [
    "We can then compare the results with the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95693b3-052c-443d-8d84-92e85377f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = np.array(KeyDataset(ds, 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c021ba79-80a4-42ed-b5b9-127b56036a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.23 %\n",
      "Bad outputs: 2\n"
     ]
    }
   ],
   "source": [
    "matches = 0\n",
    "bad_outputs = 0\n",
    "\n",
    "for label, sentiment in zip(labels, sentiments):\n",
    "    try:\n",
    "        sentiment = int(sentiment)\n",
    "        matches += (sentiment == label)\n",
    "    except ValueError as e:\n",
    "        bad_outputs += 1    \n",
    "\n",
    "print(f\"Accuracy: {100 * matches / n_reviews:.2f} %\")\n",
    "print(f\"Bad outputs: {bad_outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27201ca3-a76f-4359-96e6-a675d31109fa",
   "metadata": {},
   "source": [
    "As a comparison, we can try out a simple sentiment analysis pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62dc6fb0-3336-4d3d-b735-7c7d24c6977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b2d08b3-4c43-46e2-928f-6c5030343496",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = np.array([\n",
    "    0 if sentiment['label'] == \"NEGATIVE\" else 1\n",
    "    for sentiment in sentiment_pipeline(\n",
    "            KeyDataset(ds, 'text'),\n",
    "        )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5242725-a4f7-44ce-8fbe-71bb3dd6806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.77 %\n"
     ]
    }
   ],
   "source": [
    "matches = np.sum(labels == sentiments)\n",
    "\n",
    "print(f\"Accuracy: {100 * matches / n_reviews:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple-huggingface",
   "language": "python",
   "name": "simple-huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
